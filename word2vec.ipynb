{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bson \n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import scipy.sparse\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "from mtranslate import translate\n",
    "from langdetect import detect\n",
    "from langdetect import detect_langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda2\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2 = open(\"all_top_skills_final_key.txt\",\"rb\")\n",
    "key_list = pickle.load(f2)\n",
    "f2.close()\n",
    "f2 = open(\"all_top_skills_final_fre.txt\",\"rb\")\n",
    "fre_save = pickle.load(f2)\n",
    "f2.close()\n",
    "f= file('word2vec_model_allskills', 'rb')\n",
    "word2vec=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2 = open(\"document_per_infor.txt\",\"rb\")\n",
    "document = pickle.load(f2)\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    \\nwith open(\"linkedin_db/skill.bson\",\\'rb\\') as f: \\n    data_skill = bson.decode_all(f.read()) \\n    f.close()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"linkedin_db/profile.bson\",'rb') as f: \n",
    "    data_profile = bson.decode_all(f.read()) \n",
    "    f.close()\n",
    "'''    \n",
    "with open(\"linkedin_db/skill.bson\",'rb') as f: \n",
    "    data_skill = bson.decode_all(f.read()) \n",
    "    f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431b39efa8bb4bf4ae7dfa24639c675e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2547.48909696\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# construct phrase so that we can train them by word2vec\n",
    "############################################################\n",
    "\n",
    "File = open(\"test\", \"w\")\n",
    "start = time.clock()\n",
    "text_save=[]\n",
    "m=0\n",
    "\n",
    "for i in tqdm(range(len(data_profile))):\n",
    "#for i in tqdm(range(0,5000)):\n",
    "    if 'skills' in (data_profile[i].keys()):\n",
    "        k=0\n",
    "        for skills_divided in (data_profile[i]['skills']): \n",
    "            ## data_profile[0]['skills']是个列表，所以skills_divided就代表了topskill等的字典\n",
    "             #for skills_detail in skills_divided['skills']:  ##skills_divided['skills']又是列表\n",
    "                for title in skill_titlt:\n",
    "                    if skills_divided['title'] == title:\n",
    "                        for j in range(len(skills_divided['skills'])):\n",
    "                            for x in key_list.keys():\n",
    "                                if skills_divided['skills'][j]['title'] in key_list[x]:\n",
    "                                    if k==0:\n",
    "                                        text_save.append(x)\n",
    "                                        k = 1 #############to check if the word is the entry of this setence\n",
    "                                    else:\n",
    "                                        text_save[m] += ' '+ x\n",
    "                                    \n",
    "                        #m += 1  \n",
    "                        ##########################just one kind of skills(eg:the top skills) in one profile serve as one setence \n",
    "        m += 1\n",
    "           ##########################################all skills in one profile serve as one setence\n",
    "                        \n",
    "                        \n",
    "'''\n",
    "    if j==0:\n",
    "        text_save.append(translate(translate(skills_divided['skills'][j]['title'].encode('unicode-escape').decode('string_escape')).lower().replace(' ','_')).lower())\n",
    "    else:               \n",
    "        text_save[m] += ' '+ translate(skills_divided['skills'][j]['title'].encode('unicode-escape').decode('string_escape')).lower().replace(' ','_')\n",
    "m +=1\n",
    "'''\n",
    "\n",
    "print(time.clock()-start)                    \n",
    "File.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#f1 = open(\"skills_phrase.txt\",\"wb\")\n",
    "#pickle.dump(text_save, f1)\n",
    "#f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-10 19:11:15,095 : INFO : saving Word2Vec object under word2vec_model_allskills, separately None\n",
      "2018-12-10 19:11:15,098 : INFO : not storing attribute vectors_norm\n",
      "2018-12-10 19:11:15,101 : INFO : not storing attribute cum_table\n",
      "2018-12-10 19:11:15,302 : INFO : saved word2vec_model_allskills\n"
     ]
    }
   ],
   "source": [
    "model.save('word2vec_model_allskills')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b3c71e8c294645a444c95d928983f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-10 19:08:14,147 : INFO : collecting all words and their counts\n",
      "2018-12-10 19:08:14,161 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-12-10 19:08:14,246 : INFO : PROGRESS: at sentence #10000, processed 161324 words, keeping 4515 word types\n",
      "2018-12-10 19:08:14,282 : INFO : PROGRESS: at sentence #20000, processed 317084 words, keeping 5789 word types\n",
      "2018-12-10 19:08:14,315 : INFO : PROGRESS: at sentence #30000, processed 474016 words, keeping 6771 word types\n",
      "2018-12-10 19:08:14,322 : INFO : collected 6855 word types from a corpus of 484193 raw words and 30716 sentences\n",
      "2018-12-10 19:08:14,323 : INFO : Loading a fresh vocabulary\n",
      "2018-12-10 19:08:14,349 : INFO : effective_min_count=1 retains 6855 unique words (100% of original 6855, drops 0)\n",
      "2018-12-10 19:08:14,351 : INFO : effective_min_count=1 leaves 484193 word corpus (100% of original 484193, drops 0)\n",
      "2018-12-10 19:08:14,500 : INFO : deleting the raw counts dictionary of 6855 items\n",
      "2018-12-10 19:08:14,516 : INFO : sample=0.001 downsamples 64 most-common words\n",
      "2018-12-10 19:08:14,529 : INFO : downsampling leaves estimated 366369 word corpus (75.7% of prior 484193)\n",
      "2018-12-10 19:08:14,588 : INFO : estimated required memory for 6855 words and 100 dimensions: 8911500 bytes\n",
      "2018-12-10 19:08:14,588 : INFO : resetting layer weights\n",
      "2018-12-10 19:08:14,701 : INFO : training model with 3 workers on 6855 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-12-10 19:08:15,321 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-10 19:08:15,328 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-10 19:08:15,338 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-10 19:08:15,341 : INFO : EPOCH - 1 : training on 484193 raw words (366346 effective words) took 0.6s, 614673 effective words/s\n",
      "2018-12-10 19:08:15,730 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-10 19:08:15,736 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-10 19:08:15,746 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-10 19:08:15,747 : INFO : EPOCH - 2 : training on 484193 raw words (366643 effective words) took 0.4s, 910074 effective words/s\n",
      "2018-12-10 19:08:16,153 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-10 19:08:16,153 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-10 19:08:16,163 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-10 19:08:16,163 : INFO : EPOCH - 3 : training on 484193 raw words (366401 effective words) took 0.4s, 914265 effective words/s\n",
      "2018-12-10 19:08:16,569 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-10 19:08:16,585 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-10 19:08:16,592 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-10 19:08:16,595 : INFO : EPOCH - 4 : training on 484193 raw words (366695 effective words) took 0.4s, 857034 effective words/s\n",
      "2018-12-10 19:08:17,013 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-10 19:08:17,022 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-10 19:08:17,025 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-10 19:08:17,026 : INFO : EPOCH - 5 : training on 484193 raw words (366404 effective words) took 0.4s, 864662 effective words/s\n",
      "2018-12-10 19:08:17,029 : INFO : training on a 2420965 raw words (1832489 effective words) took 2.3s, 792496 effective words/s\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#raw_sentences = [\"the quick brown fox jumps over the lazy dogs\",\"yoyoyo you go home now to sleep\"]\n",
    "raw_sentences = text_save\n",
    "\n",
    "# divide the phrase\n",
    "#sentences= [s.encode('utf-8').split() for s in raw_sentences]\n",
    "sentences= [s.encode('utf-8').split() for s in tqdm(raw_sentences)]\n",
    "\n",
    "# train\n",
    "model = word2vec.Word2Vec(sentences, min_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda2\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('restaurant_management', 0.9657000303268433),\n",
       " ('food_service', 0.9557430744171143),\n",
       " ('business_trip', 0.9462014436721802),\n",
       " ('banquets', 0.9395477175712585),\n",
       " ('food_&_beverage', 0.9393905997276306),\n",
       " ('catering_management', 0.9344069957733154),\n",
       " ('store_management', 0.933702290058136),\n",
       " ('wine_tasting', 0.9282915592193604),\n",
       " ('leisure_travel', 0.9280977249145508),\n",
       " ('food', 0.9274133443832397)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['catering'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda2\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('apparel', 0.9573209285736084),\n",
       " ('trend_analysis', 0.9342238903045654),\n",
       " ('store_management', 0.92498779296875),\n",
       " ('image_editing', 0.9092636108398438),\n",
       " ('beauty_industry', 0.9078472852706909),\n",
       " ('purchase_orders', 0.9054895639419556),\n",
       " ('food_industry', 0.9047888517379761),\n",
       " ('catering', 0.9019890427589417),\n",
       " ('packaging', 0.9009928703308105),\n",
       " ('retail_sales', 0.8955800533294678)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['textile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda2\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('military', 0.9515854120254517),\n",
       " ('operational_planning', 0.9402337670326233),\n",
       " ('military_operations', 0.937116801738739),\n",
       " ('army', 0.9215548038482666),\n",
       " ('command', 0.916386604309082),\n",
       " ('emergency_management', 0.875515878200531),\n",
       " ('commandment', 0.8699114322662354),\n",
       " ('navy', 0.8659511804580688),\n",
       " ('government', 0.8601311445236206),\n",
       " ('national_security', 0.8479140400886536)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['defense'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda2\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('international_business', 0.9112564921379089),\n",
       " ('fmcg', 0.8954953551292419),\n",
       " ('cosmetics', 0.8946792483329773),\n",
       " ('luxury_goods', 0.8879814147949219),\n",
       " ('market_analysis', 0.876038670539856),\n",
       " ('food_technology', 0.8691227436065674),\n",
       " ('trade_marketing', 0.863317608833313),\n",
       " ('international_marketing', 0.8591989278793335),\n",
       " ('pricing', 0.8554950952529907),\n",
       " ('luxury', 0.8528965711593628)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['international_trade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387dff157f6d4d599ebe5225d16b06b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda2\\lib\\site-packages\\ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "D:\\anaconda2\\lib\\site-packages\\ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "value_save,key2save = [],[]\n",
    "for skills_divided in (data_profile[0]['skills']): \n",
    "    ## data_profile[0]['skills']是个列表，所以skills_divided就代表了topskill等的字典\n",
    "     #for skills_detail in skills_divided['skills']:  ##skills_divided['skills']又是列表\n",
    "        if skills_divided['title'] == 'Top Skills':\n",
    "            #_str=''\n",
    "            for j in ((range(len(skills_divided['skills'])))):\n",
    "                for x in key_list.keys():\n",
    "                    if skills_divided['skills'][j]['title'] in key_list[x]:\n",
    "                        #value_save.append(model[x])\n",
    "                        key2save.append(x)\n",
    "\n",
    "\n",
    "value2save,max2save = [],[]\n",
    "max_save=0\n",
    "for i in tqdm(range(len(data_profile))):\n",
    "#for i in tqdm(range(0,5000)):\n",
    "    if 'skills' in (data_profile[i].keys()):\n",
    "        for skills_divided in (data_profile[i]['skills']): \n",
    "            ## data_profile[0]['skills']是个列表，所以skills_divided就代表了topskill等的字典\n",
    "             #for skills_detail in skills_divided['skills']:  ##skills_divided['skills']又是列表\n",
    "                if skills_divided['title'] == 'Top Skills'\n",
    "                    for j in ((range(len(skills_divided['skills'])))):\n",
    "                        for x in key_list.keys():\n",
    "                            if x in model:\n",
    "                            ###############################################################need attention    \n",
    "                                if skills_divided['skills'][j]['title'] in key_list[x]:\n",
    "                                    max2save.append(max([model.similarity(key2save[k],x) for k in range(len(key2save))]))\n",
    "                    if np.linalg.norm(max2save)>np.linalg.norm(max_save):\n",
    "                        max_save = max2save\n",
    "                        index=i\n",
    "                        max2save=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09090909090909091"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = wn.synset('Defense.n.01')\n",
    "b = wn.synset('security.n.01')\n",
    "a.path_similarity(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda2\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5997746"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('security', 'ohsas18001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
